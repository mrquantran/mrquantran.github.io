<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An Introduction to Whisper Architecture with Subtitle Generator | Quan Hong Tran </title> <meta name="author" content="Quan Hong Tran"> <meta name="description" content="Whisper is a multitask, multilingual model trained on 680,000 hours of diverse audio data. Its transformer-based architecture transcribes, translates, and identifies languages directly from raw audio—all in a single end-to-end pipeline."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%BC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mrquantran.github.io//blog/2025/whisper-ai/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "An Introduction to Whisper Architecture with Subtitle Generator",
            "description": "Whisper is a multitask, multilingual model trained on 680,000 hours of diverse audio data. Its transformer-based architecture transcribes, translates, and identifies languages directly from raw audio—all in a single end-to-end pipeline.",
            "published": "February 05, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Quan</span> Hong Tran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>An Introduction to Whisper Architecture with Subtitle Generator</h1> <p>Whisper is a multitask, multilingual model trained on 680,000 hours of diverse audio data. Its transformer-based architecture transcribes, translates, and identifies languages directly from raw audio—all in a single end-to-end pipeline.</p> <p class="post-meta"> Created in February 05, 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> speech-to-text   ·   <i class="fa-solid fa-tag fa-sm"></i> simple-project </p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#core-idea">Core Idea</a> </div> <div> <a href="#core-idea">Core Idea</a> </div> <div> <a href="#architecture">Architecture</a> </div> <div> <a href="#inference">Inference</a> </div> <div> <a href="#simple-usage-example">Simple Usage Example</a> </div> </nav> </d-contents> <p>Speech-to-text technology has advanced significantly in recent years, with OpenAI’s Whisper leading the way. Whisper is an automatic speech recognition (ASR) system that utilizes advanced machine learning methods and a straightforward design to transcribe and translate speech in 99 languages. In this blog, I’ll break down how Whisper works, step by step, and demonstrate how it can be used in simple Subtitle Generator projects.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/openai_whisper/model-480.webp 480w,/assets/img/posts/openai_whisper/model-800.webp 800w,/assets/img/posts/openai_whisper/model-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/openai_whisper/model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Summary of their method: A sequence-to-sequence model, the Transformer model, is trained on various speech processing tasks, such as multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are represented as a sequence of tokens for the decoder to predict, enabling a single model to replace multiple stages of a traditional speech processing pipeline. The multitask training format employs specific tokens that act as task identifiers or classification targets. </div> </div> <h2 id="core-idea">Core Idea</h2> <p>At its core, Whisper is an <strong>encoder-decoder transformer model</strong> trained on 680,000 hours of diverse audio data. Unlike traditional Automatic Speech Recognition (ASR) systems that rely on complex pipelines with separate components for acoustic modeling and language processing, Whisper unifies these tasks into a single neural network. This approach allows it to handle accents, background noise, and multilingual inputs more robustly.</p> <p>The model’s versatility comes from its ability to perform multiple tasks—like transcription, translation, and language identification—using a unified architecture. By conditioning the decoder with special tokens (e.g., <code class="language-plaintext highlighter-rouge">&lt;|en|&gt;</code> for English or <code class="language-plaintext highlighter-rouge">&lt;|translate|&gt;</code> for translation), Whisper dynamically adapts to the user’s needs.</p> <hr> <h2 id="architecture-from-sound-waves-to-text">Architecture: From Sound Waves to Text</h2> <h3 id="step-1-processing-audio-into-features">Step 1: Processing Audio into Features</h3> <p>Whisper begins by converting raw audio into a <strong>log-Mel spectrogram</strong>, a mathematical representation that captures frequency patterns over time. This spectrogram is generated by:</p> <ol> <li> <strong>Resampling</strong>: Audio is standardized to 16,000 Hz.</li> <li> <strong>Spectrogram Creation</strong>: Splitting the waveform into 25-millisecond windows with a 10-millisecond stride, then converting these windows into 80 Mel-frequency bins (mimicking human hearing sensitivity).</li> <li> <strong>Normalization</strong>: The spectrogram is scaled to a range of [-1, 1] to stabilize training [1][2].</li> </ol> <p>This step is crucial because it transforms chaotic sound waves into a structured format the model can analyze.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/openai_whisper/mel_spectrogram-480.webp 480w,/assets/img/posts/openai_whisper/mel_spectrogram-800.webp 800w,/assets/img/posts/openai_whisper/mel_spectrogram-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/openai_whisper/mel_spectrogram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="step-2-the-encoder--understanding-speech">Step 2: The Encoder – Understanding Speech</h3> <p>The encoder outputs a high-dimensional representation of the audio, capturing both acoustic and linguistic features.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/openai_whisper/encoder_whisper-480.webp 480w,/assets/img/posts/openai_whisper/encoder_whisper-800.webp 800w,/assets/img/posts/openai_whisper/encoder_whisper-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/openai_whisper/encoder_whisper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="convolutional-layers">Convolutional Layers</h4> <p>The spectrogram first passes through two 1D convolutional layers (kernel size 3), which act as a “stem” to detect local acoustic patterns like phonemes or formants. These layers:</p> <ul> <li>Reduce dimensionality while preserving temporal resolution.</li> <li>Use GELU activation to introduce non-linearity, enabling the model to learn complex relationships in the audio [6] [33].</li> </ul> <h4 id="positional-embeddings">Positional Embeddings</h4> <p>Since audio is a time-series, Whisper adds <strong>sinusoidal positional embeddings</strong> to the spectrogram. These embeddings encode the position of each 40ms audio frame, allowing the model to understand temporal order—crucial for distinguishing sequences like “cat” vs. “act”533.</p> <h4 id="transformer-blocks">Transformer Blocks</h4> <p>The core of the encoder is a stack of 32 transformer blocks (for large models). Each block includes:</p> <ul> <li> <strong>Multi-head self-attention:</strong> Identifies relationships between distant audio segments. For example, it links pronouns (“he”) to their later references (“ran”) even if separated by seconds of speech.</li> <li> <strong>Feed-forward networks:</strong> Refine features using learned transformations.</li> <li> <strong>Pre-activation layer normalization:</strong> Stabilizes training by normalizing inputs before attention and feed-forward operations.</li> </ul> <p>Through these layers, the encoder gradually builds a high-dimensional representation (1,500 tokens of 1,280 dimensions for 30-second audio) that encapsulates both acoustic details (e.g., pitch) and linguistic context</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/openai_whisper/encoder_block-480.webp 480w,/assets/img/posts/openai_whisper/encoder_block-800.webp 800w,/assets/img/posts/openai_whisper/encoder_block-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/openai_whisper/encoder_block.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="step-3-the-decoder--generating-text">Step 3: The Decoder – Generating Text</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/openai_whisper/decoder-480.webp 480w,/assets/img/posts/openai_whisper/decoder-800.webp 800w,/assets/img/posts/openai_whisper/decoder-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/openai_whisper/decoder.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The decoder generates text <strong>one token at a time</strong> using an autoregressive process:</p> <p>The decoder predicts text tokens autoregressively, one word at a time, using:</p> <ol> <li> <table> <tbody> <tr> <td>Task-Specific Tokens: Instructions like &lt;</td> <td>transcribe</td> <td>&gt; or &lt;</td> <td>translate</td> <td>&gt; guide the decoder49.</td> </tr> </tbody> </table> </li> <li>Cross-Attention: The decoder focuses on relevant parts of the encoder’s output while generating each token.</li> <li>Language Modeling: Whisper’s training on vast text data helps it produce grammatically correct sentences, even in noisy environments1036.</li> </ol> <p>For example, when translating French to English, the decoder might process: <code class="language-plaintext highlighter-rouge">&lt;|startoftranscript|&gt;&lt;|fr|&gt;&lt;|translate|&gt;</code> → “Bonjour” → “Hello”</p> <hr> <h2 id="inference">Inference</h2> <ol> <li> <strong>Chunking:</strong> Long audio is split into 30-second segments (or padded to fit).</li> <li> <strong>Batch Processing:</strong> Multiple segments are processed in parallel for efficiency.</li> <li> <strong>Timestamp Prediction:</strong> Optional timestamps mark when each word was spoken.</li> </ol> <p>This pipeline ensures Whisper handles everything from podcasts to phone calls seamlessly.</p> <hr> <h2 id="simple-usage-example">Simple Usage Example</h2> <p>Using Whisper is straightforward with Open AI whisper Library</p> <h3 id="installation-and-setup">Installation and Setup</h3> <p>To use Whisper, install it via pip:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">pip <span class="nb">install </span>openai-whisper</code></pre></figure> <h3 id="basic-transcription">Basic Transcription</h3> <p>Here’s how to transcribe an audio file in Python:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">whisper</span>
<span class="c1"># Load the model (choose 'tiny', 'base', 'small', 'medium', or 'large')
</span><span class="n">model</span> <span class="o">=</span> <span class="n">whisper</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="sh">"</span><span class="s">base</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Transcribe audio
</span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">transcribe</span><span class="p">(</span><span class="sh">"</span><span class="s">audio.mp3</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">])</span></code></pre></figure> <ul> <li> <strong>Translation</strong>: Use <code class="language-plaintext highlighter-rouge">model.transcribe(..., task="translate")</code> to convert non-English speech to English text.</li> <li> <strong>Timestamps</strong>: Enable word-level timestamps with <code class="language-plaintext highlighter-rouge">model.transcribe(..., verbose=True)</code>.</li> </ul> <h3 id="subtitle-generator-example">Subtitle Generator Example</h3> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/subtitle-generator.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <hr> <h2 id="useful-link">Useful Link</h2> <ul> <li><a href="https://gattanasio.cc/post/whisper-encoder/" rel="external nofollow noopener" target="_blank">Whisper Encoder</a></li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'al-folio';
    var disqus_identifier = '/blog/2025/whisper-ai';
    var disqus_title      = "An Introduction to Whisper Architecture with Subtitle Generator";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Quan Hong Tran. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://rum.cronitor.io/script.js"></script> <script defer src="/assets/js/cronitor-analytics-setup.js?edde7e927b37b35ed2e06db320ee68e0"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>