<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Simple 3D Gaussian Splats from Unposed Images | Quan Hong Tran </title> <meta name="author" content="Quan Hong Tran"> <meta name="description" content="NoPoSplat introduces a feed-forward network that directly predicts 3D Gaussian parameters within a canonical space from sparse, unposed multi-view images."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%BC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mrquantran.github.io//blog/2025/no-pose-no-problem/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Quan</span> Hong Tran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Simple 3D Gaussian Splats from Unposed Images</h1> <p class="post-meta"> Created in January 20, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/gaussian-splatting"> <i class="fa-solid fa-hashtag fa-sm"></i> gaussian-splatting</a>   ·   <a href="/blog/category/paper-notes"> <i class="fa-solid fa-tag fa-sm"></i> paper-notes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <ul> <li><a href="https://noposplat.github.io/" rel="external nofollow noopener" target="_blank">NoPoSplat Project Page</a></li> <li><a href="https://quan283.notion.site/No-Pose-No-Problem-Surprisingly-Simple-3D-Gaussian-Splats-from-Sparse-Unposed-Images-1832a3f8f2a180ec8690d5080a39394f?pvs=4" rel="external nofollow noopener" target="_blank">Summary version with cited in Notion</a></li> </ul> <h1 id="1-introduction-and-problem-statement">1. Introduction and Problem Statement</h1> <p>Existing state-of-the-art (SOTA) methods for generalizable 3D scene reconstruction, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), require <strong>accurate camera poses</strong> as input. These poses are typically obtained through <strong>Structure-from-Motion (SfM)</strong> methods like COLMAP. However, this requirement is impractical in many real-world scenarios, especially when dealing with <strong>sparse views</strong> (e.g., just a couple of images) or in <strong>textureless areas</strong> where pose estimation is unreliable.</p> <h2 id="limitations-of-existing-methods">Limitations of Existing Methods</h2> <p>Current methods that attempt to jointly estimate poses and reconstruct scenes suffer from a <strong>compounding effect</strong>: errors in initial pose estimates degrade reconstruction quality, which in turn further degrades pose accuracy. This creates a feedback loop that limits the effectiveness of these approaches.</p> <h2 id="noposplats-solution">NoPoSplat’s Solution</h2> <p>NoPoSplat introduces a <strong>feed-forward network</strong> that directly predicts 3D Gaussian parameters within a <strong>canonical space</strong> from sparse, unposed multi-view images. This eliminates the need for explicit pose estimation during reconstruction. By anchoring the first input view’s local camera coordinates as the canonical space, NoPoSplat avoids the need to transform Gaussians from local to global coordinates, thus bypassing the errors associated with pose estimation.</p> <h1 id="2-related-works">2. Related Works</h1> <h2 id="21-generalizable-3d-reconstruction-and-view-synthesis">2.1. Generalizable 3D Reconstruction and View Synthesis</h2> <p>NeRF (Neural Radiance Fields) and 3DGS (3D Gaussian Splatting) have significantly advanced 3D reconstruction and novel view synthesis, but these methods typically require <strong>dense posed images</strong> as input and <strong>per-scene optimization</strong> limits their practical application</p> <p>Recent approaches focus on <strong>generalizable 3D reconstruction</strong> from sparse inputs<strong>.</strong> These methods typically use task-specific backbones that leverage <strong>geometric information</strong> to enhance scene reconstruction. Examples include:</p> <ul> <li> <strong>MVSNeRF and MuRF</strong>: These methods build <strong>cost volumes</strong> to aggregate multi-view information</li> <li> <strong>pixelSplat</strong>: This method employs <strong>epipolar geometry</strong> for improved depth estimation</li> </ul> <p>However, these geometric operations often require <strong>camera pose input</strong> and sufficient <strong>camera pose overlap</strong> among input views. In contrast, NoPoSplat uses a <strong>Vision Transformer (ViT)</strong> without any geometric priors, making it <strong>pose-free</strong> and more effective in handling scenes with <strong>large camera baselines</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="22-pose-free-3d-scene-reconstruction">2.2. Pose-Free 3D Scene Reconstruction</h2> <p>Classical methods rely on accurate camera poses of input images, typically obtained through Structure from Motion (SfM) methods like COLMAP, which complicates the overall process.</p> <p>Some recent works attempt to <strong>jointly optimize camera poses</strong> and neural scene representations, but they still require rough pose initialization or are limited to small motions. Other methods adopt <strong>incremental approaches</strong>, but they only allow image/video sequences as input</p> <p>For generalizable sparse-view methods, requiring camera poses during inference presents significant challenges, as these poses are often unavailable in real-world applications during testing. While two-view pose estimation methods can be used, they are prone to failure in textureless regions or when images lack sufficient overlap. Some recent pose-free novel view synthesis methods break the task into two stages: first estimate camera poses, then construct the scene representation. However, this two-stage process lags behind pose-required methods due to noise from the initial pose estimation.</p> <p>In contrast, NoPoSplat completely eliminates camera poses by directly predicting 3D Gaussians in <strong>a canonical space</strong>, avoiding potential noise in pose estimation and achieving better scene reconstruction</p> <p><strong>Splatt3R</strong>, a concurrent work, also predicts Gaussians in a global coordinate system but relies on the frozen <strong>MASt3R model</strong> for Gaussian centers and requires ground truth depth during training, which makes it unsuitable for novel view synthesis and limits its ability to leverage widely available video data</p> <h1 id="3-methodology">3. Methodology</h1> <h2 id="31-problem-formulation">3.1. Problem Formulation:</h2> <p>The goal is to reconstruct a 3D scene from a set of unposed multi-view images and their corresponding camera intrinsics, represented as ${I^v, k^v}$</p> <p>Here, $I^v$ is the input image for view $v$, and $k^v$ represents the camera intrinsic parameters for that view.</p> <p>The network, denoted as $f_θ$ with learnable parameters $θ$, maps these inputs to 3D Gaussians within a <strong>canonical 3D space</strong>. The mapping is defined as:</p> \[fθ : \{(I^v,k^v)\}^V_{v=1} → {∪ (\mu^v_j ,\alpha^v_j ,r^v_j , s^v_j , c^v_j )}^{v=1,...,V}_{j=1,...,H \times W}\] <ul> <li>$\mu$: the center position of the Gaussian primitive in $R^3$</li> <li>$\alpha$: the opacity of the Gaussian primitive in $R$.</li> <li>$r$: the rotation factor in quaternion in $R^4$</li> <li>$s$: the scale of the Gaussian primitive in $R^3$</li> <li>$c$: the spherical harmonics (SH) coefficients of the Gaussian primitive in $R^k$</li> </ul> <p>The method assumes that camera intrinsics (<em>k</em>) are available, which is a common assumption as they are generally available from modern devices.</p> <p>The method can generalize without any optimization. The results 3D Gaussians in canonical space enabble two tasks</p> <ol> <li> <strong>Novel view synthesis</strong>: Given a target camera transformation relative to the first input view, the model renders a novel view.</li> <li> <strong>Relative pose estimation</strong>: The model estimates the relative camera poses between input views.</li> </ol> <h2 id="32-overall-pipeline">3.2. Overall Pipeline</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The NoPoSplat pipeline consists of three main components:</p> <ol> <li> <strong>Encoder</strong>: A Vision Transformer (ViT) that processes the input images and camera intrinsics.</li> <li> <strong>Decoder</strong>: Another ViT that integrates multi-view information through cross-attention.</li> <li> <strong>Gaussian Parameter Prediction Heads</strong>: Two prediction heads that output the Gaussian parameters (center positions and other attributes).</li> </ol> <p>Both the encoder and decoder use Vision Transformer (ViT) architectures without any geometric priors, which is a key difference from other methods that rely on geometric priors like epipolar constraints or cost volumes. The use of a pure ViT structure allows the model to be more flexible and effective when the overlap between input views is limited.</p> <h3 id="321-vision-transformer-vit-encoder-and-decoder">3.2.1 Vision Transformer (ViT) Encoder and Decoder</h3> <p>The RGB images are converted into sequences of image tokens, and these are concatenated with an <strong>intrinsic token</strong> (Detail in Section 3.4). These tokens are fed into a ViT encoder separately for each view. The encoder shares weights across all input views.</p> <p>The output features of the encoder are passed to a ViT decoder, which integrates multi-view information through <strong>cross-attention layers</strong>, allowing the features from each view to interact with those from all other views.</p> <h3 id="322-gaussian-parameter-prediction-heads">3.2.2 Gaussian Parameter Prediction Heads</h3> <p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/dpt" rel="external nofollow noopener" target="_blank">DPT</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/image-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/image-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/image-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The DPT model was proposed in Vision Transformers for Dense Prediction by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun. DPT is a model that leverages the Vision Transformer (ViT) as backbone for dense prediction tasks like semantic segmentation and depth estimation. </div> <p>Two prediction heads are used to predict the Gaussian parameters, based on the <strong>DPT architecture.</strong></p> <ul> <li>The first head focuses on predicting <strong>Gaussian center positions ($µ$)</strong> and takes features exclusively from the transformer decoder.</li> <li>The second head predicts the other Gaussian parameters (α, r, s, c) and uses both the ViT decoder features and an RGB image shortcut as input.</li> </ul> <p>The RGB image shortcut allows for the direct flow of texture information which is crucial for capturing fine details. The ViT decoder features are downsampled, so they lack detailed structural information which the RGB shortcut compensates for</p> <h2 id="33-canonical-gaussian-space">3.3. Canonical Gaussian Space</h2> <p>Instead of predicting Gaussians in each local camera coordinate system and then transforming them to a world coordinate system using camera poses, NoPoSplat directly outputs Gaussians in a canonical space (Figure 5 and 6)</p> <p><strong>The local camera coordinate of the first input view is anchored as the canonical space</strong>, meaning that the camera pose for the first view is considered to be [U 0], where $U$ is a unit/identity matrix for rotation and 0 is a zero translation vector.</p> <p>All Gaussians are predicted relative to this canonical space. The network predicts the set</p> \[\{µ^{v→1}_j, r^{v→1}_j, c^{v→1}_j, α_j, s_j\}\] <p>where the superscript $v→1$ denotes the Gaussian parameters corresponding to pixel $p_j$ in view $v$, under the local camera coordinate system of view 1.</p> <p>This approach eliminates the need for camera poses and allows the network to learn the fusion of different views directly within the canonical space, which results in a more cohesive global representation</p> <h2 id="34-camera-intrinsics-embedding">3.4. Camera Intrinsics Embedding</h2> <p>The camera intrinsics ($k = [f_x, f_y, c_x, c_y]$) are crucial for resolving the scale ambiguity</p> <p>Three methods for embedding camera intrinsics are compared</p> <ol> <li> <strong>Global Intrinsic Embedding - Addition:</strong> Camera intrinsics are fed into a linear layer to obtain a global feature, which is added to the RGB image features after the patch embedding.</li> <li> <strong>Global Intrinsic Embedding - Concatenation:</strong> The global feature is treated as an intrinsic token and concatenated with the image tokens.</li> <li> <strong>Dense Intrinsic Embedding</strong>: Per-pixel camera rays are converted to higher-dimension features using spherical harmonics and are concatenated with the RGB image.</li> </ol> <p>The <strong>intrinsic token method (concatenation) performs the best.</strong> This method is not only effective at injecting camera intrinsic information but also gives the best performance.</p> <h2 id="35-training-and-inference">3.5 Training and Inference</h2> <h3 id="351-training">3.5.1 Training</h3> <p>The 3D Gaussians are used to render images at novel viewpoints, and the network is trained end-to-end using <strong>ground truth target RGB images</strong> as supervision. The loss function is a linear combination of <strong>MSE (Mean Squared Error)</strong> and <strong>LPIPS (Learned Perceptual Image Patch Similarity)</strong> loss with weights 1 and 0.05, respectively.</p> <h3 id="352-relative-pose-estimation">3.5.2 Relative Pose Estimation</h3> <p>The 3D Gaussians in the canonical space are directly used for relative pose estimation. The pose estimation is approached in two steps:</p> <ol> <li> <strong>Coarse Pose Estimation</strong>: The an initial pose estimate is obtained using the <strong>PnP (Perspective-n-Point) algorithm</strong> with <strong>RANSAC</strong>, given the Gaussian centers.</li> <li>Refinement: The initial pose is refined by rendering the scene using the estimated pose and optimizing the alignment with the input view using <strong>photometric loss</strong> and the structural part of the <strong>SSIM loss</strong>.</li> </ol> <p>The <strong>camera Jacobian</strong> is calculated to reduce computational overhead during optimization.</p> <h3 id="353-evaluation-time-pose-alignment">3.5.3 Evaluation Time Pose Alignment</h3> <blockquote> <p>“”However, 3D scene reconstruction with just two input views is inherently ambiguous as many different scenes can produce the same two images. As a result, though the scene generated by our method successfully explains the input views, it might not be exactly the same as the ground truth scene in the validation dataset.””</p> </blockquote> <p>For evaluation purposes, the camera pose for the target view is optimized to match the ground truth image by freezing the Gaussians and optimizing the target camera pose using photometric loss. This is done to compare with other baselines, but is not required for real world use (Figure 7)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="4-experiment-results-and-analysis">4. Experiment Results and Analysis</h1> <p>The experiments use the <strong>RealEstate10k (RE10K)</strong> and <strong>ACID</strong> datasets, and also test <strong>zero-shot generalization</strong> on <strong>DTU</strong> and <strong>ScanNet++</strong> datasets. To handle input images with varying camera overlaps, they generate input pairs for evaluation that are categorized based on the ratio of image overlap</p> <p>The evaluation metrics used include <strong>PSNR</strong>, <strong>SSIM</strong>, and <strong>LPIPS</strong> for novel view synthesis, and <strong>AUC (Area Under the Curve)</strong> for pose estimation.</p> <h2 id="41-novel-view-synthesis">4.1. Novel View Synthesis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 1: Novel view synthesis performance comparison on the RealEstate10k (Zhou et al., 2018) dataset. Our method largely outperforms previous pose-free methods on all overlap settings, and even outperforms SOTA pose-required methods, especially when the overlap is small. </div> <p><strong>NoPoSplat significantly outperforms all state-of-the-art pose-free methods in novel view synthesis.</strong> Methods such as DUSt3R and MASt3R struggle to fuse input views effectively due to their reliance on per-pixel depth loss, and Splatt3R inherits this limitation</p> <p>NoPoSplat achieves <strong>competitive performance with state-of-the-art pose-required methods</strong> like pixelSplat and MVSplat, and <strong>outperforms them particularly when the overlap between input images is small.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 2: Novel view synthesis performance comparison on the ACID (Liu et al., 2021) dataset. </div> <p>This improved performance is attributed to NoPoSplat’s 3D Gaussian prediction in a canonical space, as opposed to the transform-then-fuse strategy used by other methods.</p> <p>Qualitative results in Figure 4 demonstrate NoPoSplat’s ability to achieve more coherent fusion from input views, superior reconstruction with limited image overlap, and enhanced geometry reconstruction in non-overlapping regions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Qualitative comparison on RE10K (top three rows) and ACID (bottom row). Compared to baselines, we obtain: 1) more coherent fusion from input views, 2) superior reconstruction from limited image overlap, 3) enhanced geometry reconstruction in non-overlapping regions </div> <h2 id="42-relative-pose-estimation">4.2. Relative Pose Estimation</h2> <p>NoPoSplat shows strong performance in <strong>relative pose estimation</strong> on the <strong>RE10k</strong>, <strong>ACID</strong>, and <strong>ScanNet-1500</strong> datasets. The method is trained on either <strong>RE10K</strong> (denoted as Ours) or a combination of <strong>RE10K</strong> and <strong>DL3DV</strong> (denoted as Ours*).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 3 provides the pose estimation performance using the AUC metric with thresholds of 5°, 10°, and 20° across the different datasets </div> <p>The results demonstrate that performance consistently improves when scaling up training with <strong>DL3DV</strong>. <strong>NoPoSplat demonstrates superior zero-shot performance on ACID and ScanNet-1500</strong>, even outperforming <strong>RoMa</strong>, which was trained on <strong>ScanNet</strong>. The authors state that this indicates the effectiveness of the pose estimation approach and the quality of the 3D geometry produced.</p> <h2 id="43-geometry-reconstruction">4.3. Geometry Reconstruction</h2> <p>NoPoSplat outputs improved 3D Gaussians and depths when compared to state-of-the-art pose-required methods.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5 shows that MVSplat suffers from misalignment in the intersection regions of input images, and distortions in areas with limited overlap. These problems are attributed to the transform-then-fuse pipeline. </div> <p>Figure 5 shows that MVSplat suffers from misalignment in the intersection regions of input images, and distortions in areas with limited overlap. These problems are attributed to the transform-then-fuse pipeline<strong>8</strong>.</p> <p>NoPoSplat’s direct Gaussian prediction in canonical space addresses these issues. The results show that even without camera poses as input, NoPoSplat generates higher quality 3D Gaussians, resulting in improved color and depth rendering</p> <h2 id="44-cross-dataset-generalization">4.4. Cross-Dataset Generalization</h2> <p><strong>NoPoSplat exhibits superior zero-shot performance on out-of-distribution data</strong>, when trained exclusively on RealEstate10k and tested on ScanNet++ and DTU datasets. The model’s minimal geometric priors help it to adapt effectively to different scene types.</p> <p>As shown in Table 4 and Figure 6, NoPoSplat outperforms state-of-the-art pose-required methods on out-of-distribution data.</p> <p>Notably, NoPoSplat outperforms Splatt3R, even on ScanNet++ where Splatt3R was trained.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: Cross-dataset generalization. Our model can better zero-shot transfer to out-ofdistribution data than SOTA pose-required methods. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 4: Out-of-distribution performance comparison. Our method shows superior performance when zero-shot evaluation on DTU and ScanNet++ using the model solely trained on RE10k. </div> <h2 id="45-model-efficiency">4.5. Model Efficiency</h2> <p>NoPoSplat can predict 3D Gaussians from two <strong>256x256</strong> input images in <strong>0.015 seconds (66 fps)</strong> on an <strong>RTX 4090 GPU</strong>. This speed is approximately <strong>5 times faster</strong> than <strong>pixelSplat</strong> and <strong>2 times faster</strong> than <strong>MVSplat</strong>, which shows the benefits of using a standard ViT without additional geometric operations.</p> <h2 id="46-real-application">4.6. Real Application</h2> <p>NoPoSplat can be directly applied to <strong>in-the-wild unposed images</strong>, including images taken with mobile phones and frames extracted from videos generated by <strong>Sora</strong>. The results indicate potential applications of NoPoSplat in <strong>text/image to 3D scene generation</strong> pipelines.</p> <p>Figure 7 shows results from in-the-wild data, demonstrating the method’s applicability to sparse image pairs from mobile phones and frames from Sora-generated videos.</p> <h1 id="5-abalation-studies">5. Abalation Studies</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 5: Ablations. intrinsic embeddings are vital for performance and using intrinsic tokens performs the best. Adding the RGB image shortcut also improves the quality of rendered images. Our method achieves better performance compared with the poserequired per-local-view Gaussian field prediction method. </div> <h2 id="51-ablation-on-output-gaussian-space">5.1. Ablation on Output Gaussian Space</h2> <p>The authors compare NoPoSplat’s canonical Gaussian space prediction with the transform-then-fuse pipeline used by pose-required methods.</p> <p>The transform-then-fuse method predicts Gaussians in each local camera coordinate system and then transforms them to a world coordinate system using camera poses. For a fair comparison, both methods use the same backbone and head, but differ in the prediction of Gaussian space.</p> <p>Results in row (f) of Table 5 show that the <strong>pose-free canonical space prediction</strong> method outperforms the transform-then-fuse approach. Figure 8 shows that the transform-then-fuse strategy leads to ghosting artifacts in the rendering due to difficulties in aligning Gaussians from different input views.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 8: Ablations. No intrinsic results in blurriness due to scale misalignment. Without the RGB image shortcut, the rendered images are blurry in the texture-rich areas. Using the transform-then-fuse strategy causes ghosting problem. </div> <h2 id="52-ablation-on-camera-intrinsic-embedding">5.2. Ablation on Camera Intrinsic Embedding</h2> <p>Three intrinsic encoding strategies are studied, along with a scenario where no intrinsic information is provided. The results in Table 5 row (b) and Figure 8 show that <strong>not using intrinsic encodings leads to blurry results due to scale ambiguity.</strong></p> <p>The intrinsic token embedding consistently performs the best among the three proposed methods, and is used as the default setting. The three methods compared are (b) (c) (d)</p> <h2 id="53-importance-of-rgb-shortcut">5.3. Importance of RGB Shortcut</h2> <p>The impact of using an RGB image shortcut to the Gaussian parameter prediction head is studied. In addition to low-resolution ViT features, RGB images are input into the Gaussian parameter prediction head.</p> <p>Figure 8 shows that <strong>without the RGB shortcut, rendered images are blurry in texture-rich areas</strong></p> <h2 id="54-extend-to-3-input-views">5.4. Extend to 3 Input Views</h2> <p>The method is extended to use three input views to show how additional views improve performance. The third view is added between the two original input views.</p> <p>Row (g) of Table 5 shows that <strong>performance significantly improves with the inclusion of an additional view</strong></p> <h1 id="6-closing">6. Closing</h1> <p><strong>Reliance on Known Camera Intrinsics:</strong> NoPoSplat, like other pose-free methods, assumes that camera intrinsics are known. Although the authors note that heuristically set intrinsic parameters work well for in-the-wild images, they acknowledge that relaxing this requirement would improve the method’s robustness in real-world applications.</p> <p><strong>Non-Generative Model:</strong> As a feedforward model, NoPoSplat is not generative. This means it <strong>cannot reconstruct unseen regions of a scene with detailed geometry and texture.</strong> This limitation is apparent in the 2-view model, where areas not covered by the input views may not be accurately reconstructed. The authors suggest that incorporating additional input views could potentially mitigate this issue.</p> <p><strong>Limited Training Data:</strong> The current training data is limited to the RealEstate10K, ACID, and DL3DV datasets. This <strong>constrains the model’s ability to generalize to diverse in-the-wild scenarios</strong>. The authors propose training the model on larger, more diverse datasets in the future to address this.</p> <p><strong>Static Scenes:</strong> The method is currently limited to static scenes.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/gaussian-splatting/">3D Gaussian Splatting</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'al-folio';
    var disqus_identifier = '/blog/2025/no-pose-no-problem';
    var disqus_title      = "Simple 3D Gaussian Splats from Unposed Images";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Quan Hong Tran. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://rum.cronitor.io/script.js"></script> <script defer src="/assets/js/cronitor-analytics-setup.js?518d2a9879102c72afc8d4a1b59fd141"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>