<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mrquantran.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://mrquantran.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-27T18:31:40+00:00</updated><id>https://mrquantran.github.io//feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Simple 3D Gaussian Splats from Unposed Images</title><link href="https://mrquantran.github.io//blog/2025/no-pose-no-problem/" rel="alternate" type="text/html" title="Simple 3D Gaussian Splats from Unposed Images"/><published>2025-01-20T00:00:00+00:00</published><updated>2025-01-20T00:00:00+00:00</updated><id>https://mrquantran.github.io//blog/2025/no-pose-no-problem</id><content type="html" xml:base="https://mrquantran.github.io//blog/2025/no-pose-no-problem/"><![CDATA[<ul> <li><a href="https://noposplat.github.io/">NoPoSplat Project Page</a></li> <li><a href="https://quan283.notion.site/No-Pose-No-Problem-Surprisingly-Simple-3D-Gaussian-Splats-from-Sparse-Unposed-Images-1832a3f8f2a180ec8690d5080a39394f?pvs=4">Summary version with cited in Notion</a></li> </ul> <h1 id="1-introduction-and-problem-statement">1. Introduction and Problem Statement</h1> <p>Existing state-of-the-art (SOTA) methods for generalizable 3D scene reconstruction, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), require <strong>accurate camera poses</strong> as input. These poses are typically obtained through <strong>Structure-from-Motion (SfM)</strong> methods like COLMAP. However, this requirement is impractical in many real-world scenarios, especially when dealing with <strong>sparse views</strong> (e.g., just a couple of images) or in <strong>textureless areas</strong> where pose estimation is unreliable.</p> <h2 id="limitations-of-existing-methods">Limitations of Existing Methods</h2> <p>Current methods that attempt to jointly estimate poses and reconstruct scenes suffer from a <strong>compounding effect</strong>: errors in initial pose estimates degrade reconstruction quality, which in turn further degrades pose accuracy. This creates a feedback loop that limits the effectiveness of these approaches.</p> <h2 id="noposplats-solution">NoPoSplat’s Solution</h2> <p>NoPoSplat introduces a <strong>feed-forward network</strong> that directly predicts 3D Gaussian parameters within a <strong>canonical space</strong> from sparse, unposed multi-view images. This eliminates the need for explicit pose estimation during reconstruction. By anchoring the first input view’s local camera coordinates as the canonical space, NoPoSplat avoids the need to transform Gaussians from local to global coordinates, thus bypassing the errors associated with pose estimation.</p> <h1 id="2-related-works">2. Related Works</h1> <h2 id="21-generalizable-3d-reconstruction-and-view-synthesis">2.1. Generalizable 3D Reconstruction and View Synthesis</h2> <p>NeRF (Neural Radiance Fields) and 3DGS (3D Gaussian Splatting) have significantly advanced 3D reconstruction and novel view synthesis, but these methods typically require <strong>dense posed images</strong> as input and <strong>per-scene optimization</strong> limits their practical application</p> <p>Recent approaches focus on <strong>generalizable 3D reconstruction</strong> from sparse inputs<strong>.</strong> These methods typically use task-specific backbones that leverage <strong>geometric information</strong> to enhance scene reconstruction. Examples include:</p> <ul> <li><strong>MVSNeRF and MuRF</strong>: These methods build <strong>cost volumes</strong> to aggregate multi-view information</li> <li><strong>pixelSplat</strong>: This method employs <strong>epipolar geometry</strong> for improved depth estimation</li> </ul> <p>However, these geometric operations often require <strong>camera pose input</strong> and sufficient <strong>camera pose overlap</strong> among input views. In contrast, NoPoSplat uses a <strong>Vision Transformer (ViT)</strong> without any geometric priors, making it <strong>pose-free</strong> and more effective in handling scenes with <strong>large camera baselines</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.57.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="22-pose-free-3d-scene-reconstruction">2.2. Pose-Free 3D Scene Reconstruction</h2> <p>Classical methods rely on accurate camera poses of input images, typically obtained through Structure from Motion (SfM) methods like COLMAP, which complicates the overall process.</p> <p>Some recent works attempt to <strong>jointly optimize camera poses</strong> and neural scene representations, but they still require rough pose initialization or are limited to small motions. Other methods adopt <strong>incremental approaches</strong>, but they only allow image/video sequences as input</p> <p>For generalizable sparse-view methods, requiring camera poses during inference presents significant challenges, as these poses are often unavailable in real-world applications during testing. While two-view pose estimation methods can be used, they are prone to failure in textureless regions or when images lack sufficient overlap. Some recent pose-free novel view synthesis methods break the task into two stages: first estimate camera poses, then construct the scene representation. However, this two-stage process lags behind pose-required methods due to noise from the initial pose estimation.</p> <p>In contrast, NoPoSplat completely eliminates camera poses by directly predicting 3D Gaussians in <strong>a canonical space</strong>, avoiding potential noise in pose estimation and achieving better scene reconstruction</p> <p><strong>Splatt3R</strong>, a concurrent work, also predicts Gaussians in a global coordinate system but relies on the frozen <strong>MASt3R model</strong> for Gaussian centers and requires ground truth depth during training, which makes it unsuitable for novel view synthesis and limits its ability to leverage widely available video data</p> <h1 id="3-methodology">3. Methodology</h1> <h2 id="31-problem-formulation">3.1. Problem Formulation:</h2> <p>The goal is to reconstruct a 3D scene from a set of unposed multi-view images and their corresponding camera intrinsics, represented as ${I^v, k^v}$</p> <p>Here, $I^v$ is the input image for view $v$, and $k^v$ represents the camera intrinsic parameters for that view.</p> <p>The network, denoted as $f_θ$ with learnable parameters $θ$, maps these inputs to 3D Gaussians within a <strong>canonical 3D space</strong>. The mapping is defined as:</p> \[fθ : \{(I^v,k^v)\}^V_{v=1} → {∪ (\mu^v_j ,\alpha^v_j ,r^v_j , s^v_j , c^v_j )}^{v=1,...,V}_{j=1,...,H \times W}\] <ul> <li>$\mu$: the center position of the Gaussian primitive in $R^3$</li> <li>$\alpha$: the opacity of the Gaussian primitive in $R$.</li> <li>$r$: the rotation factor in quaternion in $R^4$</li> <li>$s$: the scale of the Gaussian primitive in $R^3$</li> <li>$c$: the spherical harmonics (SH) coefficients of the Gaussian primitive in $R^k$</li> </ul> <p>The method assumes that camera intrinsics (<em>k</em>) are available, which is a common assumption as they are generally available from modern devices.</p> <p>The method can generalize without any optimization. The results 3D Gaussians in canonical space enabble two tasks</p> <ol> <li><strong>Novel view synthesis</strong>: Given a target camera transformation relative to the first input view, the model renders a novel view.</li> <li><strong>Relative pose estimation</strong>: The model estimates the relative camera poses between input views.</li> </ol> <h2 id="32-overall-pipeline">3.2. Overall Pipeline</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.20.30.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The NoPoSplat pipeline consists of three main components:</p> <ol> <li><strong>Encoder</strong>: A Vision Transformer (ViT) that processes the input images and camera intrinsics.</li> <li><strong>Decoder</strong>: Another ViT that integrates multi-view information through cross-attention.</li> <li><strong>Gaussian Parameter Prediction Heads</strong>: Two prediction heads that output the Gaussian parameters (center positions and other attributes).</li> </ol> <p>Both the encoder and decoder use Vision Transformer (ViT) architectures without any geometric priors, which is a key difference from other methods that rely on geometric priors like epipolar constraints or cost volumes. The use of a pure ViT structure allows the model to be more flexible and effective when the overlap between input views is limited.</p> <h3 id="321-vision-transformer-vit-encoder-and-decoder">3.2.1 Vision Transformer (ViT) Encoder and Decoder</h3> <p>The RGB images are converted into sequences of image tokens, and these are concatenated with an <strong>intrinsic token</strong> (Detail in Section 3.4). These tokens are fed into a ViT encoder separately for each view. The encoder shares weights across all input views.</p> <p>The output features of the encoder are passed to a ViT decoder, which integrates multi-view information through <strong>cross-attention layers</strong>, allowing the features from each view to interact with those from all other views.</p> <h3 id="322-gaussian-parameter-prediction-heads">3.2.2 Gaussian Parameter Prediction Heads</h3> <p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/dpt">DPT</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/image-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/image-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The DPT model was proposed in Vision Transformers for Dense Prediction by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun. DPT is a model that leverages the Vision Transformer (ViT) as backbone for dense prediction tasks like semantic segmentation and depth estimation. </div> <p>Two prediction heads are used to predict the Gaussian parameters, based on the <strong>DPT architecture.</strong></p> <ul> <li>The first head focuses on predicting <strong>Gaussian center positions ($µ$)</strong> and takes features exclusively from the transformer decoder.</li> <li>The second head predicts the other Gaussian parameters (α, r, s, c) and uses both the ViT decoder features and an RGB image shortcut as input.</li> </ul> <p>The RGB image shortcut allows for the direct flow of texture information which is crucial for capturing fine details. The ViT decoder features are downsampled, so they lack detailed structural information which the RGB shortcut compensates for</p> <h2 id="33-canonical-gaussian-space">3.3. Canonical Gaussian Space</h2> <p>Instead of predicting Gaussians in each local camera coordinate system and then transforming them to a world coordinate system using camera poses, NoPoSplat directly outputs Gaussians in a canonical space (Figure 5 and 6)</p> <p><strong>The local camera coordinate of the first input view is anchored as the canonical space</strong>, meaning that the camera pose for the first view is considered to be [U 0], where $U$ is a unit/identity matrix for rotation and 0 is a zero translation vector.</p> <p>All Gaussians are predicted relative to this canonical space. The network predicts the set</p> \[\{µ^{v→1}_j, r^{v→1}_j, c^{v→1}_j, α_j, s_j\}\] <p>where the superscript $v→1$ denotes the Gaussian parameters corresponding to pixel $p_j$ in view $v$, under the local camera coordinate system of view 1.</p> <p>This approach eliminates the need for camera poses and allows the network to learn the fusion of different views directly within the canonical space, which results in a more cohesive global representation</p> <h2 id="34-camera-intrinsics-embedding">3.4. Camera Intrinsics Embedding</h2> <p>The camera intrinsics ($k = [f_x, f_y, c_x, c_y]$) are crucial for resolving the scale ambiguity</p> <p>Three methods for embedding camera intrinsics are compared</p> <ol> <li><strong>Global Intrinsic Embedding - Addition:</strong> Camera intrinsics are fed into a linear layer to obtain a global feature, which is added to the RGB image features after the patch embedding.</li> <li><strong>Global Intrinsic Embedding - Concatenation:</strong> The global feature is treated as an intrinsic token and concatenated with the image tokens.</li> <li><strong>Dense Intrinsic Embedding</strong>: Per-pixel camera rays are converted to higher-dimension features using spherical harmonics and are concatenated with the RGB image.</li> </ol> <p>The <strong>intrinsic token method (concatenation) performs the best.</strong> This method is not only effective at injecting camera intrinsic information but also gives the best performance.</p> <h2 id="35-training-and-inference">3.5 Training and Inference</h2> <h3 id="351-training">3.5.1 Training</h3> <p>The 3D Gaussians are used to render images at novel viewpoints, and the network is trained end-to-end using <strong>ground truth target RGB images</strong> as supervision. The loss function is a linear combination of <strong>MSE (Mean Squared Error)</strong> and <strong>LPIPS (Learned Perceptual Image Patch Similarity)</strong> loss with weights 1 and 0.05, respectively.</p> <h3 id="352-relative-pose-estimation">3.5.2 Relative Pose Estimation</h3> <p>The 3D Gaussians in the canonical space are directly used for relative pose estimation. The pose estimation is approached in two steps:</p> <ol> <li><strong>Coarse Pose Estimation</strong>: The an initial pose estimate is obtained using the <strong>PnP (Perspective-n-Point) algorithm</strong> with <strong>RANSAC</strong>, given the Gaussian centers.</li> <li>Refinement: The initial pose is refined by rendering the scene using the estimated pose and optimizing the alignment with the input view using <strong>photometric loss</strong> and the structural part of the <strong>SSIM loss</strong>.</li> </ol> <p>The <strong>camera Jacobian</strong> is calculated to reduce computational overhead during optimization.</p> <h3 id="353-evaluation-time-pose-alignment">3.5.3 Evaluation Time Pose Alignment</h3> <blockquote> <p>“”However, 3D scene reconstruction with just two input views is inherently ambiguous as many different scenes can produce the same two images. As a result, though the scene generated by our method successfully explains the input views, it might not be exactly the same as the ground truth scene in the validation dataset.””</p> </blockquote> <p>For evaluation purposes, the camera pose for the target view is optimized to match the ground truth image by freezing the Gaussians and optimizing the target camera pose using photometric loss. This is done to compare with other baselines, but is not required for real world use (Figure 7)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.28.49.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="4-experiment-results-and-analysis">4. Experiment Results and Analysis</h1> <p>The experiments use the <strong>RealEstate10k (RE10K)</strong> and <strong>ACID</strong> datasets, and also test <strong>zero-shot generalization</strong> on <strong>DTU</strong> and <strong>ScanNet++</strong> datasets. To handle input images with varying camera overlaps, they generate input pairs for evaluation that are categorized based on the ratio of image overlap</p> <p>The evaluation metrics used include <strong>PSNR</strong>, <strong>SSIM</strong>, and <strong>LPIPS</strong> for novel view synthesis, and <strong>AUC (Area Under the Curve)</strong> for pose estimation.</p> <h2 id="41-novel-view-synthesis">4.1. Novel View Synthesis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.38.47.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1: Novel view synthesis performance comparison on the RealEstate10k (Zhou et al., 2018) dataset. Our method largely outperforms previous pose-free methods on all overlap settings, and even outperforms SOTA pose-required methods, especially when the overlap is small. </div> <p><strong>NoPoSplat significantly outperforms all state-of-the-art pose-free methods in novel view synthesis.</strong> Methods such as DUSt3R and MASt3R struggle to fuse input views effectively due to their reliance on per-pixel depth loss, and Splatt3R inherits this limitation</p> <p>NoPoSplat achieves <strong>competitive performance with state-of-the-art pose-required methods</strong> like pixelSplat and MVSplat, and <strong>outperforms them particularly when the overlap between input images is small.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 2: Novel view synthesis performance comparison on the ACID (Liu et al., 2021) dataset. </div> <p>This improved performance is attributed to NoPoSplat’s 3D Gaussian prediction in a canonical space, as opposed to the transform-then-fuse strategy used by other methods.</p> <p>Qualitative results in Figure 4 demonstrate NoPoSplat’s ability to achieve more coherent fusion from input views, superior reconstruction with limited image overlap, and enhanced geometry reconstruction in non-overlapping regions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.00.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Qualitative comparison on RE10K (top three rows) and ACID (bottom row). Compared to baselines, we obtain: 1) more coherent fusion from input views, 2) superior reconstruction from limited image overlap, 3) enhanced geometry reconstruction in non-overlapping regions </div> <h2 id="42-relative-pose-estimation">4.2. Relative Pose Estimation</h2> <p>NoPoSplat shows strong performance in <strong>relative pose estimation</strong> on the <strong>RE10k</strong>, <strong>ACID</strong>, and <strong>ScanNet-1500</strong> datasets. The method is trained on either <strong>RE10K</strong> (denoted as Ours) or a combination of <strong>RE10K</strong> and <strong>DL3DV</strong> (denoted as Ours*).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.46.53.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 3 provides the pose estimation performance using the AUC metric with thresholds of 5°, 10°, and 20° across the different datasets </div> <p>The results demonstrate that performance consistently improves when scaling up training with <strong>DL3DV</strong>. <strong>NoPoSplat demonstrates superior zero-shot performance on ACID and ScanNet-1500</strong>, even outperforming <strong>RoMa</strong>, which was trained on <strong>ScanNet</strong>. The authors state that this indicates the effectiveness of the pose estimation approach and the quality of the 3D geometry produced.</p> <h2 id="43-geometry-reconstruction">4.3. Geometry Reconstruction</h2> <p>NoPoSplat outputs improved 3D Gaussians and depths when compared to state-of-the-art pose-required methods.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5 shows that MVSplat suffers from misalignment in the intersection regions of input images, and distortions in areas with limited overlap. These problems are attributed to the transform-then-fuse pipeline. </div> <p>Figure 5 shows that MVSplat suffers from misalignment in the intersection regions of input images, and distortions in areas with limited overlap. These problems are attributed to the transform-then-fuse pipeline<strong>8</strong>.</p> <p>NoPoSplat’s direct Gaussian prediction in canonical space addresses these issues. The results show that even without camera poses as input, NoPoSplat generates higher quality 3D Gaussians, resulting in improved color and depth rendering</p> <h2 id="44-cross-dataset-generalization">4.4. Cross-Dataset Generalization</h2> <p><strong>NoPoSplat exhibits superior zero-shot performance on out-of-distribution data</strong>, when trained exclusively on RealEstate10k and tested on ScanNet++ and DTU datasets. The model’s minimal geometric priors help it to adapt effectively to different scene types.</p> <p>As shown in Table 4 and Figure 6, NoPoSplat outperforms state-of-the-art pose-required methods on out-of-distribution data.</p> <p>Notably, NoPoSplat outperforms Splatt3R, even on ScanNet++ where Splatt3R was trained.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_21.53.59.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6: Cross-dataset generalization. Our model can better zero-shot transfer to out-ofdistribution data than SOTA pose-required methods. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_22.58.05.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 4: Out-of-distribution performance comparison. Our method shows superior performance when zero-shot evaluation on DTU and ScanNet++ using the model solely trained on RE10k. </div> <h2 id="45-model-efficiency">4.5. Model Efficiency</h2> <p>NoPoSplat can predict 3D Gaussians from two <strong>256x256</strong> input images in <strong>0.015 seconds (66 fps)</strong> on an <strong>RTX 4090 GPU</strong>. This speed is approximately <strong>5 times faster</strong> than <strong>pixelSplat</strong> and <strong>2 times faster</strong> than <strong>MVSplat</strong>, which shows the benefits of using a standard ViT without additional geometric operations.</p> <h2 id="46-real-application">4.6. Real Application</h2> <p>NoPoSplat can be directly applied to <strong>in-the-wild unposed images</strong>, including images taken with mobile phones and frames extracted from videos generated by <strong>Sora</strong>. The results indicate potential applications of NoPoSplat in <strong>text/image to 3D scene generation</strong> pipelines.</p> <p>Figure 7 shows results from in-the-wild data, demonstrating the method’s applicability to sparse image pairs from mobile phones and frames from Sora-generated videos.</p> <h1 id="5-abalation-studies">5. Abalation Studies</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 5: Ablations. intrinsic embeddings are vital for performance and using intrinsic tokens performs the best. Adding the RGB image shortcut also improves the quality of rendered images. Our method achieves better performance compared with the poserequired per-local-view Gaussian field prediction method. </div> <h2 id="51-ablation-on-output-gaussian-space">5.1. Ablation on Output Gaussian Space</h2> <p>The authors compare NoPoSplat’s canonical Gaussian space prediction with the transform-then-fuse pipeline used by pose-required methods.</p> <p>The transform-then-fuse method predicts Gaussians in each local camera coordinate system and then transforms them to a world coordinate system using camera poses. For a fair comparison, both methods use the same backbone and head, but differ in the prediction of Gaussian space.</p> <p>Results in row (f) of Table 5 show that the <strong>pose-free canonical space prediction</strong> method outperforms the transform-then-fuse approach. Figure 8 shows that the transform-then-fuse strategy leads to ghosting artifacts in the rendering due to difficulties in aligning Gaussians from different input views.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55-480.webp 480w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55-800.webp 800w,/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/no_pose_no_problem/Screenshot_2025-01-22_at_23.03.55.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8: Ablations. No intrinsic results in blurriness due to scale misalignment. Without the RGB image shortcut, the rendered images are blurry in the texture-rich areas. Using the transform-then-fuse strategy causes ghosting problem. </div> <h2 id="52-ablation-on-camera-intrinsic-embedding">5.2. Ablation on Camera Intrinsic Embedding</h2> <p>Three intrinsic encoding strategies are studied, along with a scenario where no intrinsic information is provided. The results in Table 5 row (b) and Figure 8 show that <strong>not using intrinsic encodings leads to blurry results due to scale ambiguity.</strong></p> <p>The intrinsic token embedding consistently performs the best among the three proposed methods, and is used as the default setting. The three methods compared are (b) (c) (d)</p> <h2 id="53-importance-of-rgb-shortcut">5.3. Importance of RGB Shortcut</h2> <p>The impact of using an RGB image shortcut to the Gaussian parameter prediction head is studied. In addition to low-resolution ViT features, RGB images are input into the Gaussian parameter prediction head.</p> <p>Figure 8 shows that <strong>without the RGB shortcut, rendered images are blurry in texture-rich areas</strong></p> <h2 id="54-extend-to-3-input-views">5.4. Extend to 3 Input Views</h2> <p>The method is extended to use three input views to show how additional views improve performance. The third view is added between the two original input views.</p> <p>Row (g) of Table 5 shows that <strong>performance significantly improves with the inclusion of an additional view</strong></p> <h1 id="6-closing">6. Closing</h1> <p><strong>Reliance on Known Camera Intrinsics:</strong> NoPoSplat, like other pose-free methods, assumes that camera intrinsics are known. Although the authors note that heuristically set intrinsic parameters work well for in-the-wild images, they acknowledge that relaxing this requirement would improve the method’s robustness in real-world applications.</p> <p><strong>Non-Generative Model:</strong> As a feedforward model, NoPoSplat is not generative. This means it <strong>cannot reconstruct unseen regions of a scene with detailed geometry and texture.</strong> This limitation is apparent in the 2-view model, where areas not covered by the input views may not be accurately reconstructed. The authors suggest that incorporating additional input views could potentially mitigate this issue.</p> <p><strong>Limited Training Data:</strong> The current training data is limited to the RealEstate10K, ACID, and DL3DV datasets. This <strong>constrains the model’s ability to generalize to diverse in-the-wild scenarios</strong>. The authors propose training the model on larger, more diverse datasets in the future to address this.</p> <p><strong>Static Scenes:</strong> The method is currently limited to static scenes.</p>]]></content><author><name></name></author><category term="paper-notes"/><category term="gaussian-splatting"/><summary type="html"><![CDATA[NoPoSplat introduces a feed-forward network that directly predicts 3D Gaussian parameters within a canonical space from sparse, unposed multi-view images.]]></summary></entry><entry><title type="html">3D Gaussian Splatting</title><link href="https://mrquantran.github.io//blog/2024/gaussian-splatting/" rel="alternate" type="text/html" title="3D Gaussian Splatting"/><published>2024-08-15T00:00:00+00:00</published><updated>2024-08-15T00:00:00+00:00</updated><id>https://mrquantran.github.io//blog/2024/gaussian-splatting</id><content type="html" xml:base="https://mrquantran.github.io//blog/2024/gaussian-splatting/"><![CDATA[<p>The paper begins by highlighting the recent revolution in novel-view synthesis using <code class="language-plaintext highlighter-rouge">Neural Radiance Field</code> methods, which renders 3D scenes from various viewpoints, given multiple images and their corresponding camera pose values</p> <p>However, it points out two key problems: achieving high visual quality requires computationally expensive neural networks for training and rendering, and faster methods often sacrifice visual quality for speed. The problem is further compounded when rendering unbounded scenes at 1080p resolution, where no existing method can achieve real-time display rates.</p> <p>This paper has become a hot topic in the <code class="language-plaintext highlighter-rouge">novel-view synthesis field</code>. It surpasses <code class="language-plaintext highlighter-rouge">Mip-NeRF360</code> (2022), the SOTA (state-of-the-art) method in rendering quality for high-resolution (1920x1080) outputs, and even reduces training time compared to <code class="language-plaintext highlighter-rouge">InstantNGP</code> (2022), the SOTA method for speed.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/compared-nerf-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/compared-nerf-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/compared-nerf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/compared-nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the diagram above, the values in parentheses represent <code class="language-plaintext highlighter-rouge">rendering speed (fps)</code>. It shows how much improvement has been achieved compared to existing SOTA studies. The prospect of real-time services using NeRF has come one step closer. (Although there was previously a method called <code class="language-plaintext highlighter-rouge">FastNeRF</code>, capable of rendering at 200 FPS, it had slow training times and lower quality.)</p> <blockquote> <p>The reason this paper has gained significant attention is its revolutionary fast rendering speed (exceeding approximately 100 FPS), in addition to the advantages mentioned above.</p> </blockquote> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/results_evalute-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/results_evalute-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/results_evalute-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/results_evalute.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="why-use-3d-gaussian-">Why use 3D Gaussian ?</h1> <p>To introduce two methods of representing 3D models:</p> <ol> <li>The most common methods are <strong>Meshes</strong> and <strong>Points</strong>. These are optimized for <strong>rasterization</strong> (the process of converting 3D into 2D images) based on GPU/CUDA.</li> <li>In recent NeRF techniques, scenes are represented as <strong>continuous scenes</strong>, which are well-suited for optimization. However, during rendering, the <strong>stochastic (probability-based) sampling</strong> process requires significant computation and may produce noise.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/Dolphin_triangle_mesh-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/Dolphin_triangle_mesh-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/Dolphin_triangle_mesh-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/Dolphin_triangle_mesh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/nerf_repersentation-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/nerf_repersentation-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/nerf_repersentation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/nerf_repersentation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Let’s explain this from a rendering perspective from existing NeRF:</p> <ol> <li>Draws a ray for each pixel of the image and samples several points along the ray.</li> <li>Calculates the color and volume density for each sampled point.</li> <li>Summates these values along the ray to render the image.</li> </ol> <p>This paper combines the advantages of the aforementioned 3D model representation methods and proposes a novel approach called <strong>3D Gaussian</strong>.</p> <ul> <li>Supports <strong>differentiable volumetric representation</strong>,</li> <li>Allows <strong>explicit representation</strong> (unlike neural networks, which use implicit representations with hidden structures),</li> <li>Efficiently performs <strong>2D projection</strong> of 3D models and <strong>α-blending</strong> (an additive operation for transparency values), enabling <strong>fast rendering</strong>.</li> </ul> <blockquote> <p>Note: 2D projection and α-blending are parts of the rasterization process</p> </blockquote> <p>From a computational perspective, the rendering operations required for 3D Gaussian Splatting are significantly lower compared to conventional NeRF.</p> <h1 id="overview">Overview</h1> <p>Let’s first take a look at the overall pipeline. Look at it to get understanding the big picture.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/overview_pipeline-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/overview_pipeline-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/overview_pipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/overview_pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li><strong>Initialization</strong>: Using an <strong>SfM (Structure-from-Motion)</strong> algorithm like COLMAP, not only the camera poses but also the point cloud data can be obtained. These point clouds are used as the initial values for the 3D Gaussians.</li> <li><strong>Projection</strong>: The 3D Gaussians are projected onto the image plane (a plane at a distance of 1 unit along the z-axis from the camera) to form 2D Gaussians. This step is performed to <em>update parameters by comparing with ground truth (GT) input images</em>.</li> <li><strong>Differentiable Tile Rasterizer</strong>: A differentiable tile rasterization process generates the 2D Gaussians into a single image.</li> <li><strong>Gradient Flow</strong>: The generated image and the GT image are used to <em>calculate the loss</em>, and gradients are backpropagated accordingly.</li> <li><strong>Adaptive Density Control</strong>: Based on the gradients, the shape of the Gaussians is adjusted.</li> </ol> <p>This paper review will be explained based on pseudocode. While it might seem rigid, this approach is chosen as it is the most effective way to explain unfamiliar concepts. The actual paper also feels like reading a written manual.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/algorithm_1-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/algorithm_1-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/algorithm_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/algorithm_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The overall process can be divided into three main parts:</p> <ol> <li><strong>The first part (red)</strong>: This is the variable initialization step. Although it may appear simple from a coding perspective, it is critical as it pertains to model design.</li> <li><strong>The second part (blue)</strong>: This structure is familiar in the ML field. It involves taking inputs, performing inference, calculating the loss, and updating parameters.</li> <li><strong>The third part (green)</strong>: This part directly manipulates the Gaussians. At specific iterations, Gaussians are cloned, split, and removed.</li> </ol> <h1 id="initialization">Initialization</h1> <p>M, S, C, and A are the learning parameters. Below is the explanation of the meaning and initial values of each parameter:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/initialization-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/initialization-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/initialization-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/initialization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>$M$ (mean): represents the point cloud obtained using SfM (Structure-from-Motion). A 3D Gaussian is composed of a mean and a covariance. The points in the point cloud are used as the initial mean values of the 3D Gaussians. As a result, the number of 3D Gaussians generated is equal to the number of points in the point cloud.</p> <p>$S$ (Covariance Matrix): is the covariance matrix of the 3D Gaussian, which is a $3 \times 3$ matrix. In the mathematical formulation in the paper, it is described as $\Sigma$, composed of a Scale Matrix (S) and a Rotation Matrix (R). (Note: the notation differs between the pseudocode and the equations in the paper, as shown below.)</p> <p>\begin{equation}\Sigma = R SS^TR^T\end{equation}</p> <ul> <li>The design separates the scaling factor $s$ and the quaternion $q$ (one of the ways used for rotation representation) into two independent factors for optimization.</li> <li>The scaling vector $s$ contains information about scaling along the $x$, $y$, and $z$ axes.</li> <li>The quaternion representation $q$ (shape: $4 \times 1$) is converted into a rotation matrix $R$ (shape: $3 \times 3$)</li> <li>The reason for this design is to ensure that the covariance matrix is positive definite when projecting the 3D Gaussian into a 2D Gaussian for rendering. A positive definite matrix ensures that all variables have values greater than 0.</li> </ul> <p>$C$ (color) represents the color value of the 3D Gaussian, and color is designed using the Spherical Harmonics (SH) function. Each 3D Gaussian is designed to find the optimal SH coefficient according to the view direction</p> <p>$A$ (transparency) represents the transparency (alpha) value of the 3D Gaussian and is a single scalar value.</p> <h1 id="differentiable-3d-gaussian-splatting">Differentiable 3D Gaussian Splatting</h1> <p>The 3D Gaussians are projected to 2D splats for rendering. . This projection is done using a viewing transformation, and a 2x2 covariance matrix $\sigma$ is obtained that has similar properties as if the method started from planar points with normals. \begin{equation}\Sigma’ = JW\Sigma W^T J^T\end{equation}</p> <ul> <li><strong>Projective Transformation</strong>: The Jacobian matrix $J$ (a matrix of partial derivatives) for converting from the camera coordinate system to the image coordinate system.</li> <li><strong>Viewing Transformation</strong>: The transformation matrix $W$ for converting from the world coordinate system to the camera coordinate system.</li> <li><strong>Covariance Matrix</strong>: $\Sigma$, the covariance matrix in the world coordinate system.</li> </ul> <h1 id="optimization">Optimization</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/optimization-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/optimization-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/optimization-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/optimization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>From this point, we delve into the operations executed within the <strong>train loop</strong>.</p> <p><strong>Line 1</strong>: Simply reads the target image $\hat{I}$ and its corresponding camera pose information $V$.</p> <p><strong>Line 2</strong>: The inputs— $M$(mean = $xyz$), $S$ (covariance), $C$(color), $A$ (transparency), and $V$ (camera pose)—are passed to the Rasterizer, which generates the predicted image.</p> <p><strong>Line 3</strong>: The predicted image is compared with the ground truth (GT) image to compute the <strong>loss</strong>. The loss function is designed as a combination of <strong>L1 loss</strong> and <strong>D-SSIM</strong>, with $\lambda = 0.2$. Additionally, for $M$(mean = $xyz$), the <strong>standard exponential decay scheduling</strong> is applied, similar to the Plenoxel approach.</p> <p>\begin{equation}L = (1-\lambda) L_1 + \lambda L_{DSSIM}\end{equation}</p> <p><strong>Line 4</strong>: The $M$, $S$, $C$, and $A$ values are updated using the Adam optimizer.</p> <h1 id="rasterizer">Rasterizer</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/rasterizer-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/rasterizer-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/rasterizer-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/rasterizer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The algorithm takes as input:</p> <ul> <li>Image dimensions (w, h),</li> <li>Gaussian means and covariances in world space (μ, Σ),</li> <li>Gaussian colors and opacities (c, α),</li> <li>Camera pose for the image (V).</li> </ul> <h2 id="cull-gaussian">Cull Gaussian:</h2> <p>The view frustum is a 3D volume representing the visible area of the camera, and the culling process determines which 3D Gaussians fall within this volume.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/view_frustum-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/view_frustum-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/view_frustum-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/view_frustum.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This step performs frustum culling to remove 3D Gaussians that are not within the current camera’s view</p> <p>The algorithm uses a 99% confidence interval of each Gaussian, keeping only the Gaussians that intersect the view frustum, and using a guard band to reject those at extreme positions. This avoids unnecessary computation on invisible or problematic Gaussians</p> <h2 id="screen-space-gaussian">Screen Space Gaussian</h2> <p>Transforms the 3D Gaussian parameters from world space to screen space, preparing them for 2D rasterization.</p> <p>The 3D Gaussian means (μ) and covariances (Σ) are transformed into camera coordinates using a viewing transformation (V) and the Jacobian of the affine approximation of the projective transformation. The result, $M’$ and $S’$, represent the means and covariances in screen space. The original 3D covariance matrix (Σ) is reduced to a 2x2 covariance matrix during the projection.</p> <p>This step involves matrix multiplication and affine transformations to change coordinate systems, specifically projectively transforming the 3D Gaussians into a 2D view space.</p> <h2 id="create-tiles">Create Tiles</h2> <p>This step divides the image space into smaller regions to enable parallel processing of different screen region. Creates a grid of tiles over the image, splitting the screen into 16x16 pixel tiles for parallel processing</p> <p>This is a straightforward partitioning of the screen into equally sized rectangular tiles.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/gaussian_view_frustum-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/gaussian_view_frustum-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/gaussian_view_frustum-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/gaussian_view_frustum.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/pixel_tiles-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/pixel_tiles-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/pixel_tiles-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/pixel_tiles.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="duplicate-with-keys">Duplicate With Keys</h2> <p>Each Gaussian in the scene ($M’$, $S’$) is duplicated for each tile it overlaps. A key is generated for each instance. This key combines the <strong>view space depth</strong> of the Gaussian and the <strong>tile ID</strong>. The key is constructed so that the lower bits represent the projected depth and the higher bits the tile ID.</p> <p>The resulting list contains the Gaussian instances and their corresponding keys (L, keys).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/duplicate_with_keys-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/duplicate_with_keys-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/duplicate_with_keys-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/duplicate_with_keys.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="sort-by-keys">Sort by Keys</h2> <p>Sorts the Gaussian instances based on the generated keys for each tile.</p> <p>A fast GPU Radix Sort is used to sort the Gaussian instances (L) using the keys (keys), ensuring the Gaussian instances are ordered by depth within each tile. The Radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping the keys by individual digits that share the same significant position and value. It provides a fast, parallel way of sorting.</p> <p>This sorting is done once for each frame and is a global sort across all tiles.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/sort_by_keys-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/sort_by_keys-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/sort_by_keys-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/sort_by_keys.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="identify---read-tile-ranges">Identify - Read Tile Ranges</h2> <p>Creates per-tile lists of Gaussians, by identifying start and end indices within the sorted keys array. By comparing neighboring elements of the sorted keys array, it identifies the start and end indices for each tile. The result, R, is a set of per-tile lists of Gaussians.</p> <p>Read the range r for each tile in the image.</p> <h2 id="blend-in-order">Blend In Order</h2> <p>Blends the colors and opacities of the sorted Gaussians for each pixel within the tile to create the final pixel color. The Gaussians in the tile’s range, as defined in range, are blended front-to-back. The color and opacity of each Gaussian that overlaps the current pixel are combined with existing pixel color and opacity using alpha-blending. If the accumulated opacity of the pixel is close to 1, the loop is terminated early.</p> <p>The contribution of a Gaussian is determined by a 2D Gaussian function with covariance $Σ’$, and a learned per-point opacity.</p> <p>During the rasterization process, the algorithm also ensures numerical stability by</p> <ul> <li>Skipping blending updates with alpha values lower than a small threshold.</li> <li>Clamping the accumulated alpha values.</li> <li>Stopping the blending operation early if the accumulated opacity of the pixel exceeds a threshold.</li> </ul> <h1 id="adaptive-controlof-gaussians">Adaptive Control of Gaussians</h1> <p>This is the stage where 3D Gaussians are adaptively modified to suit the scene. While the previously mentioned parameters $M, S, C, A$ are updated in each iteration, the processes in the green section are executed every 100 iterations. In this stage, 3D Gaussians undergo removal, splitting, or cloning, collectively referred to as densification.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/adaptive_density_control-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/adaptive_density_control-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/adaptive_density_control-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/adaptive_density_control.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/figure_adaptive_control-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/figure_adaptive_control-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/figure_adaptive_control-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/figure_adaptive_control.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="remove-gaussian">Remove Gaussian</h2> <p>Gaussians with an alpha value ($\alpha$, transparency) below a specific threshold ($\varepsilon$) are removed. In the code, this threshold is set to 0.005</p> <p>After removing Gaussians step, the system addresses regions where geometric features are <strong>not well-captured</strong> (=Under-reconstruction) and regions where Gaussians have modeled overly broad areas (=Over-reconstruction). Specifically:</p> <ul> <li>Both under-reconstruction and over-reconstruction regions are identified as having a <strong>high view-space positional gradient</strong>.</li> <li>If the <strong>average magnitude</strong> of the view-space positional gradient exceeds a certain <strong>threshold</strong> (set at <strong>0.0002</strong>), the Gaussians in these regions are subjected to <strong>cloning</strong> or <strong>splitting</strong>.</li> </ul> <h2 id="clone-gaussian">Clone Gaussian</h2> <p>For <strong>under-reconstructed regions</strong>:</p> <ul> <li><strong>Small Gaussians</strong> (Gaussians with <strong>low covariance</strong>) are <strong>duplicated</strong>.</li> <li>The cloned Gaussians are placed <strong>along the direction of the positional gradient</strong>.</li> </ul> <h2 id="split-gaussian">Split Gaussian</h2> <p>For <strong>over-reconstructed regions</strong>:</p> <ul> <li><strong>Large Gaussians</strong> (Gaussians with <strong>high covariance</strong>) are <strong>split into smaller Gaussians</strong>.</li> <li>A single Gaussian is divided into <strong>two Gaussians</strong>, and the scale is reduced by a factor of <strong>1.6</strong> (determined experimentally).</li> <li>The positions of the split Gaussians are assigned based on the <strong>probability density function</strong> of the original Gaussian.</li> </ul> <h2 id="optimization-1">Optimization</h2> <p>In the case of <strong>cloning</strong>, both the number of Gaussians and the scene’s volume increase. On the other hand, with <strong>splitting</strong>, the total volume is maintained while the number of Gaussians increases. As with other volumetric techniques, <strong>floaters</strong> appear in regions closer to the camera, manifesting as randomly scattered Gaussians.</p> <p>To address this, a strategy is employed where the <strong>alpha values are reset to 0 every 3000 iterations</strong>. During the stage where $M$, $S$, $C$, and $A$ are optimized, the alpha values transition from 0 to non-zero values over <strong>100 iterations</strong>. After these 100 iterations, the unwanted values are removed through the <strong>Remove Gaussian</strong> operation during the <strong>Gaussian densification</strong> phase.</p> <p>Another benefit of this approach is that it also addresses cases where 3D Gaussians overlap. By periodically resetting the alpha values to 0, instances of large Gaussians overlapping are effectively eliminated.</p> <p>The strategy of periodically resetting alpha values to 0 plays a significant role in controlling the overall number of Gaussians.</p> <h1 id="evaluation">Evaluation</h1> <p>Mip-NeRF360 utilized <strong>4 A100 GPUs</strong>, whereas the others used <strong>A6000 GPUs</strong>. FPS refers to the rendering speed. Compared to Instant-NGP, <strong>3D Gaussian Splatting</strong> achieves a similar training speed but higher <strong>PSNR</strong> and, most importantly, <strong>significantly faster rendering speed</strong>.</p> <p>From the perspective of rendering speed, Instant-NGP’s <strong>~10 FPS</strong> might seem acceptable, but this speed was achieved using high-spec GPUs. If run on low-cost GPUs, <strong>stuttering</strong> could occur.</p> <p>Drawbacks:</p> <ul> <li>Unlike previous methods, this approach consumes <strong>a significant amount of memory</strong>.</li> <li>During training for very large scenes, <strong>GPU memory usage reached up to 20GB</strong>.</li> <li>The source code explicitly recommends using GPUs with at least <strong>24GB of VRAM</strong>.</li> <li>For smaller scenes, GPUs with less memory can be used.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/quality-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/quality-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/quality-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/quality.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="ablation-study">Ablation Study</h1> <h2 id="initialization-from-sfm">Initialization from SfM</h2> <p>This section addresses experiments on the initialization of 3D Gaussians using the <strong>SfM point cloud</strong>. When creating a cube three times the size of the input camera’s bounding box and sampling it uniformly, it was found that the method performed relatively well without completely failing, even in the absence of SfM points. As shown in the figure below, performance degradation is mainly observed in the background.</p> <p>In areas not covered by the training views, initializing with random values results in floaters that cannot be eliminated through optimization. From another perspective, synthetic NeRF datasets do not exhibit this issue because they lack backgrounds, and the input camera pose values are well-defined.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/ablation_sfm-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/ablation_sfm-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/ablation_sfm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/ablation_sfm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="densification">Densification</h2> <p>They evaluated the densification method from the perspectives of cloning and splitting. Each method was disabled individually, while the others were optimized without modification. As seen in the figure below, splitting large Gaussians helps reconstruct the background effectively, whereas cloning small Gaussians enables faster convergence.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper-notes/gaussian_splatting/ablation_densification-480.webp 480w,/assets/img/paper-notes/gaussian_splatting/ablation_densification-800.webp 800w,/assets/img/paper-notes/gaussian_splatting/ablation_densification-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper-notes/gaussian_splatting/ablation_densification.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="limitations">Limitations</h2> <p>Artifacts can occur in sparse scenes with insufficient input images. While anisotropic Gaussians offer many advantages, they can sometimes result in elongated artifacts or blotchy Gaussians. (This phenomenon has also been observed in other studies.) When creating large Gaussians through optimization, artifacts occasionally appear, primarily in areas that exhibit different appearances depending on the viewing pose.</p> <ul> <li>One reason for these artifacts is the trivial rejection of Gaussians during the rasterizer stage through a guard band. Using a more theoretical culling approach could mitigate these artifacts.</li> <li>Another reason lies in the simple visibility algorithm, which can lead to abrupt switching in depth/blending order of Gaussians. This issue could potentially be addressed with anti-aliasing, which has been left as a topic for future research.</li> </ul> <p>The algorithm proposed in this paper currently cannot incorporate any form of regularization. Introducing regularization could help better handle unseen areas and regions prone to artifacts.</p> <p>Although the same hyperparameters are used across all evaluations, initial experiments suggest that reducing the position learning rate might be necessary for convergence in very large scenes (e.g., urban datasets).</p> <p>Compared to prior point-based approaches, this method is highly compact, but it still uses significantly more memory than NeRF-based solutions. Training large scenes can cause peak GPU memory usage to exceed 20 GB. However, implementing low-level optimization logic could substantially reduce this. Rendering a trained scene requires sufficient GPU memory to store the entire model, with additional memory demands for the rasterizer ranging from 30 to 5000 MB depending on the scene size and image resolution.</p>]]></content><author><name></name></author><category term="paper-notes"/><category term="gaussian-splatting"/><summary type="html"><![CDATA[3D Gaussian splatting is a technique used in the field of real-time radiance field rendering.]]></summary></entry></feed>